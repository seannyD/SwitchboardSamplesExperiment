p.value = sum(perm.r >= true.r) / length(perm.r)
z.value = (true.r - mean(perm.r)) / sd(perm.r)
###########
# Stratified permutation
# Demonstrate tapply with stratification
myLetters   <-      c('x','z','d','a','e','o')
vowelOrConsonant <- c('c','c','c','v','v','v')
sample(myLetters)
tapply(myLetters, vowelOrConsonant, sample)
unlist(tapply(myLetters, vowelOrConsonant, sample))
unlist(tapply(myLetters, vowelOrConsonant, sample))
unlist(tapply(myLetters, vowelOrConsonant, sample))
# Function to sample within continents
# Note: assumes that data is sorted by continents
stratSample = function(X,groups){
unlist(tapply(X, groups, sample))
}
# permute the data and measure the correlation once
strat.perm = function(){
perm = stratSample(d$greenberg, d$continent)
cor(d$fate, perm)
}
set.seed(123)
strat.perm.r = replicate(1000, strat.perm())
hist(strat.perm.r,xlim=c(-0.6,0.6),xaxt='n')
axis(1,pos=0)
abline(v=true.r,col=2)
# Work out p value and z-value
p.value.strat = sum(strat.perm.r >= true.r) / length(strat.perm.r)
z.value.strat = (true.r - mean(strat.perm.r)) / sd(strat.perm.r)
z.value
p.value
z.value.strat
p.value.strat
hist(strat.perm.r)
abline(v=true.r,col=2)
hist(d$greenberg)
hist(d$greenberg,main="",xlab='Greenberg Diversity')
hist(d$fate,main="",xlab='Greenberg Diversity')
hist(d$fate,main="",xlab='Greenberg Diversity', breaks=10)
hist(d$fate,main="",xlab='Greenberg Diversity', breaks=20)
?hist(d$fate,main="",xlab='Greenberg Diversity', breaks=20)
hist(d$fate,main="",xlab='Greenberg Diversity', breaks=40)
hist(d$fate,main="",xlab='Greenberg Diversity', breaks=30)
hist(d$fate,main="",xlab='Greenberg Diversity', breaks=50)
hist(d$fate,main="",xlab='Greenberg Diversity', breaks=20)
hist(d$fate,main="",xlab='Road Fatalities', breaks=20)
#Load collectivism as dCM, transport stats as d
library(pcalg)
library(party)
source("myNode_barplot.r")
#careful - d$country has space after country name
setwd("~/Documents/MPI/CausalGraphs/CausalGraphExamples/Diversity")
dCM = read.csv("Data/CollectivismMigrationData_May.csv",stringsAsFactors=F)
d = read.table('Data/transportStats.txt',sep='\t',header=TRUE,stringsAsFactors=F)
d$country = gsub(" $","",d$country)
dCM = dCM[!is.na(dCM$Nation),]
dCM[dCM$Nation=="Korea",]$Nation = "Korea South"
dCM[dCM$Nation=="Russia",]$Nation = "Russian Federation"
dCM[dCM$Nation=="USA",]$Nation = "United States"
d[,names(dCM)] = dCM[match(d$country,dCM$Nation),]
cont = read.csv("Data/Countries-Continents-csv.csv", stringsAsFactors = F)
d[!is.na(d$fate),]$country[!d[!is.na(d$fate),]$country %in% cont$Country]
d$continent = cont[match(d$country,cont$Country),]$Continent
d$continent2 = cont[match(d$country,cont$Country),]$Continent2
dout = d[c('country',"fate",'greenberg','continent2')]
names(dout)[names(dout)=="continent2"] = "continent"
dout= dout[complete.cases(dout),]
write.csv(dout,"~/Documents/MPI/CausalGraphs/CausalGraphExamples/Diversity/TransportStats_Min.csv")
set.seed(129)
dout2 = dout[order(dout$continent),]
dout2$fate = dout2$greenberg
dout2$fate = unlist(tapply(dout2$fate,dout2$continent, function(X){
jitter(X,amount=(rnorm(1)*0.1)+2.5) + (rnorm(1)*0.2)
}))
dout2$fate = dout2$fate*20 + min(dout2$fate) + 2
write.csv(dout2,"~/Documents/MPI/CausalGraphs/CausalGraphExamples/Diversity/TransportData.csv")
hist(dout2$fate)
#Load collectivism as dCM, transport stats as d
library(pcalg)
library(party)
source("myNode_barplot.r")
#careful - d$country has space after country name
setwd("~/Documents/MPI/CausalGraphs/CausalGraphExamples/Diversity")
dCM = read.csv("Data/CollectivismMigrationData_May.csv",stringsAsFactors=F)
d = read.table('Data/transportStats.txt',sep='\t',header=TRUE,stringsAsFactors=F)
d$country = gsub(" $","",d$country)
dCM = dCM[!is.na(dCM$Nation),]
dCM[dCM$Nation=="Korea",]$Nation = "Korea South"
dCM[dCM$Nation=="Russia",]$Nation = "Russian Federation"
dCM[dCM$Nation=="USA",]$Nation = "United States"
d[,names(dCM)] = dCM[match(d$country,dCM$Nation),]
cont = read.csv("Data/Countries-Continents-csv.csv", stringsAsFactors = F)
d[!is.na(d$fate),]$country[!d[!is.na(d$fate),]$country %in% cont$Country]
d$continent = cont[match(d$country,cont$Country),]$Continent
d$continent2 = cont[match(d$country,cont$Country),]$Continent2
dout = d[c('country',"fate",'greenberg','continent2')]
names(dout)[names(dout)=="continent2"] = "continent"
dout= dout[complete.cases(dout),]
write.csv(dout,"~/Documents/MPI/CausalGraphs/CausalGraphExamples/Diversity/TransportStats_Min.csv")
set.seed(129)
dout2 = dout[order(dout$continent),]
dout2$fate = dout2$greenberg
dout2$fate = unlist(tapply(dout2$fate,dout2$continent, function(X){
jitter(X,amount=(rnorm(1)*0.1)+2.5) + (rnorm(1)*0.2)
}))
dout2$fate = ((dout2$fate + min(dout2$fate))*20) + 2
write.csv(dout2,"~/Documents/MPI/CausalGraphs/CausalGraphExamples/Diversity/TransportData.csv")
hist(dout2$fate)
min(dout2$fate)
#Load collectivism as dCM, transport stats as d
library(pcalg)
library(party)
source("myNode_barplot.r")
#careful - d$country has space after country name
setwd("~/Documents/MPI/CausalGraphs/CausalGraphExamples/Diversity")
dCM = read.csv("Data/CollectivismMigrationData_May.csv",stringsAsFactors=F)
d = read.table('Data/transportStats.txt',sep='\t',header=TRUE,stringsAsFactors=F)
d$country = gsub(" $","",d$country)
dCM = dCM[!is.na(dCM$Nation),]
dCM[dCM$Nation=="Korea",]$Nation = "Korea South"
dCM[dCM$Nation=="Russia",]$Nation = "Russian Federation"
dCM[dCM$Nation=="USA",]$Nation = "United States"
d[,names(dCM)] = dCM[match(d$country,dCM$Nation),]
cont = read.csv("Data/Countries-Continents-csv.csv", stringsAsFactors = F)
d[!is.na(d$fate),]$country[!d[!is.na(d$fate),]$country %in% cont$Country]
d$continent = cont[match(d$country,cont$Country),]$Continent
d$continent2 = cont[match(d$country,cont$Country),]$Continent2
dout = d[c('country',"fate",'greenberg','continent2')]
names(dout)[names(dout)=="continent2"] = "continent"
dout= dout[complete.cases(dout),]
write.csv(dout,"~/Documents/MPI/CausalGraphs/CausalGraphExamples/Diversity/TransportStats_Min.csv")
set.seed(129)
dout2 = dout[order(dout$continent),]
dout2$fate = dout2$greenberg
dout2$fate = unlist(tapply(dout2$fate,dout2$continent, function(X){
jitter(X,amount=(rnorm(1)*0.1)+2.5) + (rnorm(1)*0.2)
}))
dout2$fate = ((dout2$fate - min(dout2$fate))*20) + 2
write.csv(dout2,"~/Documents/MPI/CausalGraphs/CausalGraphExamples/Diversity/TransportData.csv")
min(dout2$fate)
hist(dout2$fate)
# set the working directory
# (this will depend on your system)
setwd("~/Documents/Teaching/QMSS2017/gitfiles/qmss-2017/permutationExample/analysis/")
# Load the data
d = read.csv("../data/TransportData.csv")
# Plot the real data
plot(d$fate, d$greenberg, xlab='Road Fatalities', ylab='Linguistic Diversity')
abline(lm(d$greenberg~ d$fate), col=2, lwd=2)
# Calculate the true correlation
true.r = cor(d$fate, d$greenberg)
# Demonstrate sample function
x = c(1,2,3,4,5)
sample(x)
sample(x)
# A function to permute the data
perm = function(){
cor(d$fate, sample(d$greenberg))
}
# set the random seed
set.seed(666)
# use repliate to permute the data many times
perm.r = replicate(1000, perm())
# Plot the distribution of permuted data
hist(perm.r)
abline(v=true.r,col=2)
# Work out p value and z-value
p.value = sum(perm.r >= true.r) / length(perm.r)
z.value = (true.r - mean(perm.r)) / sd(perm.r)
###########
# Stratified permutation
# Demonstrate tapply with stratification
myLetters   <-      c('x','z','d','a','e','o')
vowelOrConsonant <- c('c','c','c','v','v','v')
sample(myLetters)
tapply(myLetters, vowelOrConsonant, sample)
unlist(tapply(myLetters, vowelOrConsonant, sample))
unlist(tapply(myLetters, vowelOrConsonant, sample))
unlist(tapply(myLetters, vowelOrConsonant, sample))
# Function to sample within continents
# Note: assumes that data is sorted by continents
stratSample = function(X,groups){
unlist(tapply(X, groups, sample))
}
# permute the data and measure the correlation once
strat.perm = function(){
perm = stratSample(d$greenberg, d$continent)
cor(d$fate, perm)
}
set.seed(123)
strat.perm.r = replicate(1000, strat.perm())
hist(strat.perm.r)
abline(v=true.r,col=2)
# Work out p value and z-value
p.value.strat = sum(strat.perm.r >= true.r) / length(strat.perm.r)
z.value.strat = (true.r - mean(strat.perm.r)) / sd(strat.perm.r)
z.value
p.value
z.value.strat
p.value.strat
---
title: "A case for systematic sound symbolism in pragmatics: Supporting information (All data)"
output:
pdf_document:
includes:
in_header: header.tex
toc: true
---
```{r echo=F}
getMEText = function(r,ef, wald=NULL){
AIC = r[2,]$AIC
loglikDiff = signif(diff(r$logLik),2)
chi = round(r$Chisq[2],2)
df = r$`Chi Df`[2]
p = signif(r$`Pr(>Chisq)`[2],2)
wald.text = ""
if(!is.null(wald)){
est = signif(wald[1],2)
stder = signif(wald[2],2)
t = signif(wald[3],2)
wptext = ""
if(!is.na(wald[4])){
wptext = paste(", Wald p =",signif(wald[4],2))
}
wald.text = paste("beta = ",est,", std.err = ",stder, ", Wald t = ",t,wptext,';')
}
begin = 'There was no significant'
if(p <0.1){
begin = "There was a marginal"
}
if(p < 0.05){
begin = 'There was a significant'
}
return(paste(begin,ef,"(",wald.text,"log likelihood difference =",
loglikDiff,", df = ",df,", Chi Squared =", chi,", p = ",p,")."))
}
```
# Introduction
This is an analysis of an experiment into whether people can predict if an upcoming turn is a question or a statement, based on the previous turn type and the first phoneme of the target turn.
Participants listened to a series of audio samples.  Each audio sample was made up of a *context* by speaker 1 (Statement or Inititating turn) and a *response* by speaker 2.  The response was either no audio, a single segment [w] or a single semgent other than [w].
# Load libraries
```{r warning=FALSE, message=FALSE}
library(lme4)
library(lattice)
library(gplots)
library(ggplot2)
library(sjPlot)
library(party)
library(Rmisc)
library(dplyr)
```
Function for converting from logit scale
```{r}
logit2per = function(X){
return(exp(X)/(1+exp(X)))
}
```
```{r echo=FALSE}
# Set working directory
setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/InitialPhonemeExperiment/SwitchboardSamplesExperiment/processing/")
```
# Load data
```{r}
d = read.csv("../Data/Lab_and_Online_data_Processed.csv")
```
Each row in the data is a single response from a participant to a single sample.  The key variables are:
-  *partID*:  identifies participants
-  *contextSample*: The name of the audio sample used for the context.
-  *responseSample*: The name of the audio sample used for the response.
-  *responsePhoneme*: The first segment of the response.
-  *responseType*: Whether the first segment of the response came from a question or statement.
-  *answer*: The participant's response to "Is the next turn a question?"
Make *answer* a binary variable.
```{r}
d$answer = d$answer=="Yes"
d$lastAnswer = d$lastAnswer=="Yes"
```
Relevel response phoneme and context.
```{r}
d$responsePhoneme = relevel(d$responsePhoneme, 'other')
d$context = relevel(d$context, 'ST')
```
Center trial number, so that the intercept will reflect probabilities in the middle of the experiment.
```{r}
d$trialNumber.center = d$trialNumber - 25
```
## Data exclusion
We exclude participant 13 because they took much longer than other participants.
```{r}
d = d[as.character(d$partID)!="13",]
```
Are there any samples that look like outliers?  Make a basic model:
```{r cache=T}
m3 = glmer(
answer ~ 1 +
(1  | ESource /partID) +
(1 | contextSample) +
(1 | responseSample),
data = d,
family = binomial,
control = glmerControl(optimizer="bobyqa", optCtrl = list(maxfun=2e4))
)
```
Then look at the random effects.
```{r}
dotplot(ranef(m3))[[2]]
```
The sample "IN 18" is an outlier.  However, models have convergence problems when leaving it out.
```{r}
# Commented out - not run
#d = d[d$contextSample != 'IN 18',]
```
The data has `r nrow(d)` observations:
```{r}
# Number of observations per participant
table(d$partID)
table(d$context, d$responsePhoneme )
```
Exclude missing data
```{r}
d = d[complete.cases(d[,c(
"answer","trialNumber.center",
"context","responsePhoneme",
"context.sex","response.sex"
)]),]
```
load("../results/FinalModel.Rdat")
d$responsePhoneme = relevel(d$responsePhoneme,'none')
sumStats = group_by(d, partID ,context,responsePhoneme ) %>%
summarise(mean =mean(answer))
sumStats2 = summarySE(sumStats, measurevar="mean", groupvars=c("context","responsePhoneme"))
sumStats2$upper = sumStats2$mean + sumStats2$ci
sumStats2$lower = sumStats2$mean - sumStats2$ci
dodge <- position_dodge(width=0.5)
hues = seq(15, 375, length = 3 + 1)
hues = hcl(h = hues, l = 65, c = 100)[1:3]
sumStats2$shape = c("a",'b','c','a','b','c')
main.plot <- ggplot(sumStats2,
aes(x = responsePhoneme, y = mean, colour=context)) +
geom_point(mapping=aes(shape=shape, size=8),show.legend=F) + #geom_line(aes(group=context)) +
geom_errorbar(aes(ymax=mean+ci, ymin=mean-ci), width=0.25) +
xlab("First Phoneme in Response") +
ylab("Proportion of 'Question' responses") +
coord_cartesian(ylim=c(0,1)) +
scale_color_discrete(breaks=c("ST","IN"),
labels=c("Non-Initiating","Initiating"),
name="Context")
main.plot
d$context = relevel(d$context, 'ST')
d$responsePhoneme = relevel(d$responsePhoneme,'none')
sumStats = group_by(d, partID ,context,responsePhoneme ) %>%
summarise(mean =mean(answer))
sumStats2 = summarySE(sumStats, measurevar="mean", groupvars=c("context","responsePhoneme"))
sumStats2$upper = sumStats2$mean + sumStats2$ci
sumStats2$lower = sumStats2$mean - sumStats2$ci
dodge <- position_dodge(width=0.5)
hues = seq(15, 375, length = 3 + 1)
hues = hcl(h = hues, l = 65, c = 100)[1:3]
sumStats2$shape = c("a",'b','c','a','b','c')
main.plot <- ggplot(sumStats2,
aes(x = responsePhoneme, y = mean, colour=context)) +
geom_point(mapping=aes(shape=shape, size=8),show.legend=F) + #geom_line(aes(group=context)) +
geom_errorbar(aes(ymax=mean+ci, ymin=mean-ci), width=0.25) +
xlab("First Phoneme in Response") +
ylab("Proportion of 'Question' responses") +
coord_cartesian(ylim=c(0,1)) +
scale_color_discrete(breaks=c("ST","IN"),
labels=c("Non-Initiating","Initiating"),
name="Context")
main.plot
sumStats
dim(d)
sumStats = group_by(d, partID ,context,responsePhoneme ) %>%
summarise(mean =mean(answer))
sumStats
summarySE(sumStats, measurevar="mean", groupvars=c("context","responsePhoneme"))
---
title: "A case for systematic sound symbolism in pragmatics: Supporting information (All data)"
output:
pdf_document:
includes:
in_header: header.tex
toc: true
---
```{r echo=F}
getMEText = function(r,ef, wald=NULL){
AIC = r[2,]$AIC
loglikDiff = signif(diff(r$logLik),2)
chi = round(r$Chisq[2],2)
df = r$`Chi Df`[2]
p = signif(r$`Pr(>Chisq)`[2],2)
wald.text = ""
if(!is.null(wald)){
est = signif(wald[1],2)
stder = signif(wald[2],2)
t = signif(wald[3],2)
wptext = ""
if(!is.na(wald[4])){
wptext = paste(", Wald p =",signif(wald[4],2))
}
wald.text = paste("beta = ",est,", std.err = ",stder, ", Wald t = ",t,wptext,';')
}
begin = 'There was no significant'
if(p <0.1){
begin = "There was a marginal"
}
if(p < 0.05){
begin = 'There was a significant'
}
return(paste(begin,ef,"(",wald.text,"log likelihood difference =",
loglikDiff,", df = ",df,", Chi Squared =", chi,", p = ",p,")."))
}
```
# Introduction
This is an analysis of an experiment into whether people can predict if an upcoming turn is a question or a statement, based on the previous turn type and the first phoneme of the target turn.
Participants listened to a series of audio samples.  Each audio sample was made up of a *context* by speaker 1 (Statement or Inititating turn) and a *response* by speaker 2.  The response was either no audio, a single segment [w] or a single semgent other than [w].
# Load libraries
```{r warning=FALSE, message=FALSE}
library(lme4)
library(lattice)
library(gplots)
library(ggplot2)
library(sjPlot)
library(party)
library(Rmisc)
library(dplyr)
```
Function for converting from logit scale
```{r}
logit2per = function(X){
return(exp(X)/(1+exp(X)))
}
```
```{r echo=FALSE}
# Set working directory
setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/InitialPhonemeExperiment/SwitchboardSamplesExperiment/processing/")
```
# Load data
```{r}
d = read.csv("../Data/Lab_and_Online_data_Processed.csv")
```
Each row in the data is a single response from a participant to a single sample.  The key variables are:
-  *partID*:  identifies participants
-  *contextSample*: The name of the audio sample used for the context.
-  *responseSample*: The name of the audio sample used for the response.
-  *responsePhoneme*: The first segment of the response.
-  *responseType*: Whether the first segment of the response came from a question or statement.
-  *answer*: The participant's response to "Is the next turn a question?"
Make *answer* a binary variable.
```{r}
d$answer = d$answer=="Yes"
d$lastAnswer = d$lastAnswer=="Yes"
```
Relevel response phoneme and context.
```{r}
d$responsePhoneme = relevel(d$responsePhoneme, 'other')
d$context = relevel(d$context, 'ST')
```
Center trial number, so that the intercept will reflect probabilities in the middle of the experiment.
```{r}
d$trialNumber.center = d$trialNumber - 25
```
## Data exclusion
We exclude participant 13 because they took much longer than other participants.
```{r}
d = d[as.character(d$partID)!="13",]
```
Are there any samples that look like outliers?  Make a basic model:
```{r cache=T}
m3 = glmer(
answer ~ 1 +
(1  | ESource /partID) +
(1 | contextSample) +
(1 | responseSample),
data = d,
family = binomial,
control = glmerControl(optimizer="bobyqa", optCtrl = list(maxfun=2e4))
)
```
Then look at the random effects.
```{r}
dotplot(ranef(m3))[[2]]
```
The sample "IN 18" is an outlier.  However, models have convergence problems when leaving it out.
```{r}
# Commented out - not run
#d = d[d$contextSample != 'IN 18',]
```
The data has `r nrow(d)` observations:
```{r}
# Number of observations per participant
table(d$partID)
table(d$context, d$responsePhoneme )
```
Exclude missing data
```{r}
d = d[complete.cases(d[,c(
"answer","trialNumber.center",
"context","responsePhoneme",
"context.sex","response.sex"
)]),]
d$responsePhoneme = relevel(d$responsePhoneme,'none')
sumStats = group_by(d, partID ,context,responsePhoneme ) %>%
summarise(mean =mean(answer))
sumStats
d$partID
d$context
d$responsePhoneme
sumStats = group_by(d, partID ,context,responsePhoneme ) %>%
summarise(mean =mean(answer))
sumStats
